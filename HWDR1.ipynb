{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a25f278-806f-429a-a9ae-e8ca87885ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# df_train = pd.read_csv('./data/raw/mnist_train.csv')\n",
    "# df_test = pd.read_csv('./data/raw/mnist_train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c92c8faa-292d-4216-90d1-192fa03a3c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train_data = np.array(df_train)\n",
    "# # train_data.shape()\n",
    "# df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808c5c5d-8c71-4466-abab-0d3765354fb2",
   "metadata": {},
   "source": [
    "\n",
    "- 28 x 28 pixel image flattened to 784 column feature, each feature being one pixel intensity value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37ccc376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_train.columns[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "869676c6-acbe-4ad1-82bd-e9a0d22f5056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = np.array(df_train)\n",
    "# # m training examples and n features\n",
    "# train_data.shape\n",
    "# m, n = train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d613124-e30d-4e56-a89b-1986e5657e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = (train_data[:, 1:].T)/255\n",
    "# y_train = train_data[:, 0]\n",
    "# # X_train.shape  #((784, 60000)\n",
    "# # y_train.shape  (60000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0d063c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train[1:5]\n",
    "# print(y_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7169bee2-b099-413c-88ec-35eaebab477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETED\n",
    "def parameter_init(L):\n",
    "    parameters = {}\n",
    "\n",
    "    for i in range(len(L) - 1):\n",
    "        parameters[\"W\" + str(i+1)] = np.random.randn(L[i+1], L[i]) * 0.05\n",
    "        parameters[\"b\" + str(i+1)] = np.zeros((L[i+1], 1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d793ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETED\n",
    "def forward_linear(A, W, b):\n",
    "\n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "\n",
    "    return Z, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cf2ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETED\n",
    "def activation_fn(Z, act_fn):\n",
    "    if act_fn == 'relu':\n",
    "        return np.maximum(0, Z), Z\n",
    "    \n",
    "    elif act_fn == 'sigmoid':\n",
    "        A = 1 / (1 + np.exp(-Z))\n",
    "        return A, Z\n",
    "    elif act_fn == 'softmax':\n",
    "        Z_shifted = Z - np.max(Z, axis=0, keepdims=True)  # for numerical stability\n",
    "        exp_Z = np.exp(Z_shifted)\n",
    "        A = exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n",
    "        return A, Z\n",
    "    # ADD MORE ACTIVATION FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3def7bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETED\n",
    "def forward_activation(A_prev, W, b, activation):\n",
    "    \"\"\" This Function calculates the forward prop for one layer and stores the activation and cache containin (A_prev, W, b)\"\"\"\n",
    "\n",
    "     # capture both Z and also the A,W,b\n",
    "    Z, linear_cache = forward_linear(A_prev, W, b)\n",
    "    # activation_cache captures the Z value     \n",
    "    A, activation_cache = activation_fn(Z, activation)\n",
    "    \n",
    "    \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    # cache = [\n",
    "    #          ((A, W, b), Z)\n",
    "    #          ]\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62935e82-e7e4-432f-a18d-f9d0f345ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETED\n",
    "def forward_propagation(parameters, X, activations):\n",
    "    \"\"\" FWD prop \"\"\"\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    caches = []\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = forward_activation(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], activation = activations[0])\n",
    "        caches.append(cache)\n",
    "        \n",
    "\n",
    "    AL, cache  = forward_activation(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], activation = activations[1])\n",
    "    caches.append(cache)\n",
    "\n",
    "    #  caches  = [\n",
    "    #             ((A, W, b), Z) ---> Layer 1\n",
    "    #             ((A, W, b), Z) ---> layer 2\n",
    "    #            .\n",
    "    #            .\n",
    "    #            .\n",
    "    #            ((A, W, b), Z) ---> layer L\n",
    "    #            ]\n",
    "    \n",
    "    return AL, caches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8feabc2-52ea-472a-b606-6a46b8be918d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETED\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"Computes Cost using Categorical Cross Entropy Loss Function\n",
    "\n",
    "    Cost = (-1/m) * Sum for all examples -[Y*log(A)]\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    logar = np.multiply(Y, np.log(AL))\n",
    "    \n",
    "    cost = (np.sum(logar))/(-m)\n",
    "    print(f\"AL (example 3): {AL[:, 2]}\")\n",
    "    print(f\"Y (example 3): {Y[:, 2]}\")\n",
    "    print(f\"Sum of softmax probabilities for example 3: {np.sum(AL[:, 2])}\")\n",
    "    # cost = np.squeeze(cost) # to return just correct cost shape (float)\n",
    "\n",
    "    # if iterations % 100 ==0 and print_cost == True:\n",
    "    #     print(f\"Iteration: {iterations}./n Cost : {cost}\") \n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e387b26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAN ADD MORE ACTIVATION DERIVATIVES\n",
    "def act_derivatives(activation, cache):\n",
    "    Z = cache\n",
    "    if activation == 'sigmoid':\n",
    "        s = 1 / (1 + np.exp(-Z))\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    elif activation == 'relu':\n",
    "        dZ = np.array(Z > 0, dtype=float)\n",
    "        return dZ\n",
    "    elif activation == 'tanh':\n",
    "        tanh = np.tanh(Z)\n",
    "        dZ = 1 - tanh**2\n",
    "        return dZ\n",
    "    elif activation == 'leaky_relu':\n",
    "        dZ = np.where(Z > 0, 1, 0.01)\n",
    "        return dZ\n",
    "    elif activation == 'softmax':\n",
    "        # Softmax derivative is usually handled together with cross-entropy loss,\n",
    "        # but if needed, return Jacobian (only for academic or special purposes)\n",
    "        # This is a simplified softmax derivative per element (diagonal of Jacobian)\n",
    "        exps = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "        softmax = exps / np.sum(exps, axis=0, keepdims=True)\n",
    "        dZ = softmax * (1 - softmax)  # Only valid if used element-wise\n",
    "        return dZ\n",
    "\n",
    "    # More Activation Derivates can be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c257e851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETED\n",
    "def linear_backprop(dZ, cache):\n",
    "    \"\"\" This Function computes the linear backward gradients which is the same irrespective of activation functions\n",
    "      as Z = (W).(A_prev) + (b), '() -linear relationship' \n",
    "    \"\"\"\n",
    "\n",
    "    A_prev, W, b = cache\n",
    "    # No of inputs from prev activation\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1/m) * np.sum(dZ, axis = 1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "     \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c13ff068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETED\n",
    "def activation_backprop(dA, cache, activation):\n",
    "\n",
    "    linear_cache,activation_cache = cache\n",
    "\n",
    "    # Passing activation cache which is Z\n",
    "    dZ = np.multiply(dA, act_derivatives(activation, activation_cache))\n",
    "    # Passing linear cache which is A, W, b\n",
    "    dA_prev, dW, db = linear_backprop(dZ, linear_cache)\n",
    "\n",
    "   \n",
    "    # More activations can be defined\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d41d8d6-c57a-4e28-a2d7-8d2c321739a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETE\n",
    "def back_propagation(AL, Y, caches, activations):\n",
    "    \"\"\" Calculates the gradient values for each parameter and returns a dictionary of them \"\"\"\n",
    "\n",
    "    gradients = {}\n",
    "    L = len(caches)\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    if activations[1] == \"softmax\":\n",
    "        dAL = AL - Y  \n",
    "        dZ = dAL                                           # for softmax + categorical crossentropy\n",
    "    elif activations[1] == \"sigmoid\":\n",
    "        dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))    # for sigmoid + binary crossentropy\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported output activation: {activations[1]}\")\n",
    "\n",
    "    # # Derivative for the final output layer activation with CATEGORICAL CROSS ENTROPY LOSS\n",
    "    # epsilon = 1e-12\n",
    "    # AL = np.clip(AL, epsilon, 1. -epsilon)\n",
    "    # dAL = -(np.divide(Y, AL))                        # Some formula depending on the Cost Function.\n",
    "\n",
    "    # Index 0, 1, 2, 3... L so last layer is L-1                          \n",
    "    last_cache = caches[L-1]\n",
    "    # Output layer( L'th layer) gradients take dAL, cache containing (A_prev, W, b), activation of the ouput.\n",
    "    # This is a single seperate computation because of unique dAL.\n",
    "    linear_cache,activation_cache = last_cache\n",
    "    # dA_prev_temp, dW_temp, db_temp = activation_backprop(dAL, last_cache, activation = activations[1])\n",
    "    dA_prev_temp, dW_temp, db_temp = linear_backprop(dZ, linear_cache)\n",
    "\n",
    "    gradients[\"dA\" + str(L-1)] = dA_prev_temp\n",
    "    gradients[\"dW\" + str(L)] = dW_temp\n",
    "    gradients[\"db\" + str(L)] = db_temp\n",
    "\n",
    "\n",
    "    # Now we have the dA_prev from the last layer and this can be used to compute all the gradients from (L-1)layer\n",
    "    # in a single for loop because of the same activation.\n",
    "    for l in reversed(range(L-1)):\n",
    "        cache = caches[l]\n",
    "        # From L-1 layer we have the same back prop till the first layer.\n",
    "        # The reversed range gives L-2, L-3, ..... 2,1,0.\n",
    "        dA_prev_temp, dW_temp, db_temp = activation_backprop(gradients[\"dA\" + str(l+1)], cache=cache, activation = activations[0])\n",
    "        gradients[\"dA\" + str(l)] = dA_prev_temp\n",
    "        gradients[\"dW\" + str(l+1)] = dW_temp\n",
    "        gradients[\"db\" + str(l+1)] = db_temp\n",
    "\n",
    "    # gradients = {\n",
    "    #     \"dW2\": dW2,\n",
    "    #     \"db2\": db2,\n",
    "    #     \"dW1\": dW1,\n",
    "    #     \"db1\": db1,\n",
    "    # }\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "307f473c-b8c3-482b-a7ad-2bd28d5ce76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETED\n",
    "def gradient_descent(parameters, grads, learning_rate = 0.02):\n",
    "    \"\"\" This function applies gradient descent, updates the weight and bias parameters using their respective gradients.\n",
    "     \n",
    "      \n",
    "       Returns - parameters\n",
    "    \"\"\"\n",
    "\n",
    "    parameters = copy.deepcopy(parameters)\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l +1)] = parameters[\"W\" + str(l +1)] - (learning_rate * grads[\"dW\" + str(l+1)])\n",
    "        parameters[\"b\" + str(l +1)] = parameters[\"b\" + str(l +1)] - (learning_rate * grads[\"db\" + str(l+1)])\n",
    "    \n",
    "    # parameters[\"W1\"] -= learning_rate * dW1\n",
    "    # parameters[\"b1\"] -= learning_rate * db1 \n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ba3a1dc-b981-4fb2-bdec-e0b6958f2e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_NN(X, Y, n_h, activations, iterations = 900, learning_rate = 0.03, print_cost = False):\n",
    "    \"\"\" This function takes the input X and output label Y and performs\n",
    "            -forward propagation,\n",
    "            -computes cost, and applies \n",
    "            -back propagation to calculate the gradients and \n",
    "            -updates the parameters \n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    L = [X.shape[0], *n_h]\n",
    "    parameters = parameter_init(L)\n",
    "    for iter in range(iterations):\n",
    "\n",
    "        AL, caches = forward_propagation(parameters, X, activations = activations)\n",
    "        # Print the cost for every 100 iterations.\n",
    "        if iter % 100 == 0 and print_cost == True:\n",
    "            cost = compute_cost(AL, Y)\n",
    "            print(\"Cost after iteration {}: {}\".format(iter, np.squeeze(cost))) \n",
    "        # Computing gradients\n",
    "        grads = back_propagation(AL, Y, caches, activations)\n",
    "        # Parameter update\n",
    "        parameters = gradient_descent(parameters, grads, learning_rate)\n",
    "        \n",
    "\n",
    "    return parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4c998c",
   "metadata": {},
   "source": [
    "- X_train.shape  #((784, 60000)\n",
    "- y_train.shape  (60000,)\n",
    "#### Now the shapes of input are correct and change the target label y to [1,0,0,0...] format for their respective value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39a50222-1f27-4145-85c1-ff33e07d9a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_y(Y):\n",
    "    enc_y = np.zeros((Y.max() + 1, Y.size))\n",
    "    # Placing 1 according to the label (Y value, index of size m)\n",
    "    enc_y[Y, np.arange(Y.size)] = 1\n",
    "    return enc_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40316e8f",
   "metadata": {},
   "source": [
    "### Completed with encoding the target label now design the prediction and accuracy label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32a86baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_y(X: np.ndarray, parameters: dict[str, np.ndarray], activations) -> np.ndarray:\n",
    "\n",
    "    AL,caches = forward_propagation(parameters, X, activations= activations)\n",
    "    y_pred = np.argmax(AL, axis = 0)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d81ea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(true_y, predicted_y):\n",
    "    return np.mean(true_y == predicted_y)\n",
    "    # print(\"ACCURACY :\", accuracy)\n",
    "    # print(f\"Your Neural Networks Prediction is {predicted_y}\")\n",
    "    # print(f\"The actual Label of the given input is {true_y}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fd6c432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL (example 3): [0.10096703 0.10025725 0.10204066 0.09860601 0.09913204 0.10088912\n",
      " 0.09949888 0.10057553 0.09902804 0.09900543]\n",
      "Y (example 3): [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Sum of softmax probabilities for example 3: 1.0\n",
      "Cost after iteration 0: 2.3036966048972913\n",
      "AL (example 3): [0.09919359 0.10356958 0.10042135 0.09832545 0.10082086 0.09216396\n",
      " 0.09844728 0.10932095 0.09717127 0.10056571]\n",
      "Y (example 3): [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Sum of softmax probabilities for example 3: 1.0\n",
      "Cost after iteration 100: 2.2400459406317657\n",
      "AL (example 3): [0.01245518 0.04164405 0.06827169 0.0282301  0.32512956 0.03661728\n",
      " 0.13096957 0.08283245 0.07386674 0.19998337]\n",
      "Y (example 3): [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Sum of softmax probabilities for example 3: 1.0\n",
      "Cost after iteration 200: 1.206882986085008\n",
      "AL (example 3): [0.00366474 0.00112528 0.0117322  0.00243467 0.58109061 0.02616084\n",
      " 0.18582079 0.00662527 0.05967845 0.12166714]\n",
      "Y (example 3): [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Sum of softmax probabilities for example 3: 0.9999999999999999\n",
      "Cost after iteration 300: 0.6230168301030276\n",
      "AL (example 3): [0.00965279 0.0009993  0.03181644 0.00827881 0.46951585 0.03831692\n",
      " 0.1130086  0.00860911 0.14194269 0.17785948]\n",
      "Y (example 3): [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Sum of softmax probabilities for example 3: 1.0\n",
      "Cost after iteration 400: 0.48118472170390875\n",
      "AL (example 3): [0.00606319 0.0007187  0.04185111 0.017149   0.46085377 0.02192891\n",
      " 0.04359331 0.00972787 0.14837654 0.24973761]\n",
      "Y (example 3): [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Sum of softmax probabilities for example 3: 0.9999999999999999\n",
      "Cost after iteration 500: 0.4222257688552765\n",
      "AL (example 3): [4.28605181e-03 4.55559720e-04 4.05378266e-02 3.19977505e-02\n",
      " 4.69507789e-01 1.52245370e-02 1.91956453e-02 8.89742626e-03\n",
      " 1.07536828e-01 3.02360585e-01]\n",
      "Y (example 3): [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Sum of softmax probabilities for example 3: 1.0\n",
      "Cost after iteration 600: 0.38505867877380373\n",
      "AL (example 3): [3.24053162e-03 2.92785730e-04 3.38838495e-02 5.01665137e-02\n",
      " 4.91354732e-01 1.21864303e-02 1.00689121e-02 7.14123124e-03\n",
      " 7.14583515e-02 3.20206662e-01]\n",
      "Y (example 3): [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Sum of softmax probabilities for example 3: 1.0\n",
      "Cost after iteration 700: 0.3577570850701576\n",
      "The accuracy of the NN in Training : 0.9031\n",
      "\n",
      "The accuracy of the NN in Test : 0.9034\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    df_train = pd.read_csv('./data/raw/mnist/mnist_train.csv')\n",
    "    df_test = pd.read_csv('./data/raw/mnist/mnist_test.csv')\n",
    "    train_data = np.array(df_train)\n",
    "    test_data = np.array(df_test)\n",
    "\n",
    "    n_h = [32,16,10]\n",
    "    activations = ['relu', 'softmax']\n",
    "    # m training examples and n features\n",
    "    train_data.shape\n",
    "    m, n = train_data.shape\n",
    "    X_train = (train_data[:, 1:].T)/255\n",
    "    y_train = train_data[:, 0]\n",
    "    # X_train.shape  #((784, 60000)\n",
    "    # y_train.shape  (60000,)\n",
    "    \n",
    "    X_test = (test_data[:, 1:].T)/255\n",
    "    y_test = test_data[:, 0]\n",
    "\n",
    " \n",
    "    y_train_enc = encode_y(y_train)\n",
    "    params = train_NN(X_train, y_train_enc, n_h, activations, iterations = 800, learning_rate = 0.1, print_cost = True)\n",
    "\n",
    "    train_predictions = predict_y(X_train, params, activations=activations)\n",
    "    train_accuracy = accuracy(y_train, train_predictions)\n",
    "    print(f\"The accuracy of the NN in Training : {train_accuracy:.4f}\\n\")\n",
    "\n",
    "    \n",
    "    test_predictions = predict_y(X_test, params, activations=activations)\n",
    "    test_accuracy = accuracy(y_test, test_predictions)\n",
    "    print(f\"The accuracy of the NN in Test : {test_accuracy:.4f}\")\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7ece3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a0a0ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b135d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
